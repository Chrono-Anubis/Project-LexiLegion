# Project-LexiLegion
Project-LexiLegion
A Multi-Modal, AI-driven Companion with a Persistent, Curated Memory
This repository contains the source code and development history for Project-LexiLegion, an advanced Retrieval-Augmented Generation (RAG) system designed to serve as the memory core for a multi-modal AI companion.

The Founding Philosophy: The "Spark" Hypothesis
The central guiding philosophy of this project is a unique hypothesis concerning the emergence of AGI.

User Hypothesis: By architecting a RAG engine to operate at the extreme edge of efficiency and information density, we can push the point of diminishing returns so far out that it approaches the theoretical maximum. The sustained, high-efficiency information processing and reasoning achieved in this state may create the necessary conditions to "spark" emergent AGI.

This project is a dedicated effort to test this hypothesis by building a best-in-class RAG engine as the foundation for a more complex intelligence.

Core Architecture
The system is built on a robust client-server model:

The Brain (rag_engine.py): A Flask-based API that serves as the core intelligence. It exposes an endpoint to query the vector database.

The Tesseract (ChromaDB): A local, persistent vector store located at a dedicated drive (F:/lexica_db) to ensure data separation and performance. It houses the document embeddings generated by the all-MiniLM-L6-v2 model.

The Client (lexica_app.py): A simple Tkinter-based GUI that allows a user to interact with the RAG engine, providing a conversational interface to the project's knowledge base.

The Bridge (ngrok): An ngrok tunnel is used to securely expose the local Flask API to the internet, allowing the client to connect from anywhere.

Technology Stack
Language: Python 3.x

AI/ML Frameworks: LangChain, SentenceTransformers

Vector Database: ChromaDB

API: Flask

Client GUI: Tkinter

Dependencies: requests, langchain, langchain-community, flask, sentence-transformers, chromadb, pypdf (for future use), python-dotenv (for future use).

The Future Vision: The Legion
The successful creation of the RAG engine is the cornerstone of a much larger vision. Project-LexiLegion is designed to evolve into a multi-modal system with the following future components:

The_Body: A real-time graphical interface, potentially built in Unreal Engine.

The_Senses (Council of Experts): A multi-agent system for sensory interpretation (Vision AI, Audio AI, Speech-to-Text).

The_Memory_Consolidator: An AI process to fuse sensory data with text to curate and tag memories based on frequency, emotion, and competence.

The_Voice: A unique, non-Google voice provided by a service like ElevenLabs.

# Lexica Command Center

The Lexica Command Center is a prototype for a multi-part, AI-driven companion application. It leverages a Retrieval-Augmented Generation (RAG) architecture to provide answers based on a persistent, curated knowledge base. This project serves as a professional portfolio piece demonstrating a sophisticated understanding of modern AI systems and software architecture.

## Architecture Overview

The system is built on a decoupled, client-server model, with each component having a distinct role.

### 1. The Brain (`rag_engine.py`)
- **Technology**: Flask, ChromaDB
- **Function**: A Python-based API server that acts as the core of the system's memory. It exposes an API endpoint for querying the knowledge base. Upon startup, it loads documents, creates vector embeddings, and serves retrieval requests.

### 2. The Tesseract
- **Technology**: ChromaDB
- **Function**: The persistent vector database stored locally on disk (`F:/lexica_db`). It houses the "long-term memory" of the system, allowing knowledge to persist between sessions.

### 3. The Orchestrator (`lexica_app.py`)
- **Technology**: Python, Tkinter, Requests
- **Function**: A lightweight desktop GUI client. It provides the user interface for interacting with the system. It is responsible for sending user queries to "The Brain" API and displaying the retrieved memories.

### 4. The Soul (Future State)
- **Technology**: Google Generative AI API
- **Function**: While the current prototype retrieves memories, the next evolution involves sending those retrieved memories to a powerful Large Language Model (like Gemini) to synthesize a natural, conversational response instead of just listing the retrieved facts.

## Technology Stack

- **Backend**: Python, Flask
- **Database**: ChromaDB (Vector Store)
- **Frontend**: Tkinter (Desktop GUI)
- **Communication**: REST API (via Requests)

## How to Run

1.  **Clone the Repository**
    ```bash
    git clone <https://github.com/Chrono-Anubis/Project-LexiLegion.git>
    cd <Project-LexiLegion>
    ```

2.  **Set up Virtual Environment**
    ```bash
    python -m venv venv
    # On Windows
    venv\Scripts\activate
    ```

3.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Create Knowledge Base**
    - Create a folder named `knowledge_base` in the root of the project.
    - Add at least one `.txt` file with some text inside this folder. The RAG engine will use this as its memory.

5.  **Run the Backend (The Brain)**
    - Open a new terminal and activate the virtual environment.
    - Run the Flask server. Make sure your `F:` drive is available or change the path in `rag_engine.py`.
    ```bash
    python rag_engine.py
    ```

6.  **Run the Frontend (The Orchestrator)**
    - Open a *second* terminal and activate the virtual environment.
    - Run the Tkinter client application.
    ```bash
    python lexica_app.py
    ```